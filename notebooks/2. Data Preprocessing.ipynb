{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTERNAL_DATA_PATH = '../data/external'\n",
    "RAW_DATA_PATH = '../data/raw'\n",
    "PROCESSED_DATA_PATH = '../data/processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('{}/train.csv'.format(RAW_DATA_PATH))\n",
    "test_data = pd.read_csv('{}/test.csv'.format(RAW_DATA_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>source</th>\n",
       "      <th>pornografi</th>\n",
       "      <th>sara</th>\n",
       "      <th>radikalisme</th>\n",
       "      <th>pencemaran_nama_baik</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[QUOTE=jessepinkman16;5a50ac34d89b093f368b456e...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@verosvante kita2 aja nitizen yang pada kepo,t...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"#SidangAhok smg sipenista agama n ateknya mat...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@bolususulembang.jkt barusan baca undang2 ini....</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bikin anak mulu lu nof \\nkaga mikir apa kasian...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text     source  pornografi  \\\n",
       "0  [QUOTE=jessepinkman16;5a50ac34d89b093f368b456e...     kaskus           0   \n",
       "1  @verosvante kita2 aja nitizen yang pada kepo,t...  instagram           0   \n",
       "2  \"#SidangAhok smg sipenista agama n ateknya mat...    twitter           0   \n",
       "3  @bolususulembang.jkt barusan baca undang2 ini....  instagram           0   \n",
       "4  bikin anak mulu lu nof \\nkaga mikir apa kasian...     kaskus           0   \n",
       "\n",
       "   sara  radikalisme  pencemaran_nama_baik  \n",
       "0     0            0                     1  \n",
       "1     0            0                     0  \n",
       "2     1            1                     1  \n",
       "3     0            0                     0  \n",
       "4     0            0                     0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Preprocessing Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translate text-based emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Translate emoticon\n",
    "emoticon_data_path = '{}/emoticon.txt'.format(EXTERNAL_DATA_PATH)\n",
    "emoticon_df = pd.read_csv(emoticon_data_path, sep='\\t', header=None)\n",
    "emoticon_dict = dict(zip(emoticon_df[0], emoticon_df[1]))\n",
    "\n",
    "def translate_emoticon(t):\n",
    "    for w, v in emoticon_dict.items():\n",
    "        pattern = re.compile(re.escape(w))\n",
    "        match = re.search(pattern,t)\n",
    "        if match:\n",
    "            t = re.sub(pattern,v,t)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before : seru deh acaranya :-)\n",
      "After  : seru deh acaranya Bahagia\n"
     ]
    }
   ],
   "source": [
    "sample_text = 'seru deh acaranya :-)'\n",
    "\n",
    "print('Before :', sample_text)\n",
    "print('After  :', translate_emoticon(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove excessive newline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_newline(text):\n",
    "    return re.sub('\\n', ' ',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before : hari ini\n",
      "hari\n",
      "sabtu\n",
      "After  : hari ini hari sabtu\n"
     ]
    }
   ],
   "source": [
    "sample_text = 'hari ini\\nhari\\nsabtu'\n",
    "\n",
    "print('Before :', sample_text)\n",
    "print('After  :', remove_newline(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove kaskus formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_kaskus_formatting(text):\n",
    "    text = re.sub('\\[', ' [', text)\n",
    "    text = re.sub('\\]', '] ', text)\n",
    "    text = re.sub('\\[quote[^ ]*\\].*?\\[\\/quote\\]', ' ', text)\n",
    "    text = re.sub('\\[[^ ]*\\]', ' ', text)\n",
    "    text = re.sub('&quot;', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before : [QUOTE=jessepinkman16;5a50ac34d89b093f368b456e]yoiii cuy halo halo bandung[/QUOTE]\n",
      "After  :    yoiii cuy halo halo bandung   \n"
     ]
    }
   ],
   "source": [
    "sample_text = '[QUOTE=jessepinkman16;5a50ac34d89b093f368b456e]yoiii cuy halo halo bandung[/QUOTE]'\n",
    "\n",
    "print('Before :', sample_text)\n",
    "print('After  :', remove_kaskus_formatting(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    return re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before : kemaren kacao bet de www.instagram.com/lele/lili\n",
      "After  : kemaren kacao bet de \n"
     ]
    }
   ],
   "source": [
    "sample_text = 'kemaren kacao bet de www.instagram.com/lele/lili'\n",
    "\n",
    "print('Before :', sample_text)\n",
    "print('After  :', remove_url(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove excessive whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_excessive_whitespace(text):\n",
    "    return re.sub('  +', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before : budi      pergi ke           pasar\n",
      "After  : budi pergi ke pasar\n"
     ]
    }
   ],
   "source": [
    "sample_text = 'budi      pergi ke           pasar'\n",
    "\n",
    "print('Before :', sample_text)\n",
    "print('After  :', remove_excessive_whitespace(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "def tokenize_text(text, punct=False):\n",
    "    text = WordPunctTokenizer().tokenize(text)\n",
    "    text = [word for word in text if punct or word.isalnum()]\n",
    "    text = ' '.join(text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before : kemarin,aku pergi ke dagas.terus ketemu sama Ilham.\n",
      "After  : kemarin aku pergi ke dagas terus ketemu sama Ilham\n"
     ]
    }
   ],
   "source": [
    "sample_text = 'kemarin,aku pergi ke dagas.terus ketemu sama Ilham.'\n",
    "\n",
    "print('Before :', sample_text)\n",
    "print('After  :', tokenize_text(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform slang words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "slang_words = pd.read_csv('{}/slangword.csv'.format(EXTERNAL_DATA_PATH))\n",
    "slang_dict = dict(zip(slang_words['original'],slang_words['translated']))\n",
    "\n",
    "def transform_slang_words(text):\n",
    "    word_list = text.split()\n",
    "    word_list_len = len(word_list)\n",
    "    transformed_word_list = []\n",
    "    i = 0\n",
    "    while i < word_list_len:\n",
    "        if (i + 1) < word_list_len:\n",
    "            two_words = ' '.join(word_list[i:i+2])\n",
    "            if two_words in slang_dict:\n",
    "                transformed_word_list.append(slang_dict[two_words])\n",
    "                i += 2\n",
    "                continue\n",
    "        transformed_word_list.append(slang_dict.get(word_list[i], word_list[i]))\n",
    "        i += 1\n",
    "    return ' '.join(transformed_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before : siap mas sebentar lagi saya sampai 7an\n",
      "After  : siap mas sebentar lagi saya sampai tujuan\n"
     ]
    }
   ],
   "source": [
    "sample_text = 'siap mas sebentar lagi saya sampai 7an'\n",
    "\n",
    "print('Before :', sample_text)\n",
    "print('After  :', transform_slang_words(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove non aplhabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_alphabet(text):\n",
    "    output = re.sub('[^a-zA-Z ]+', '', text)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before : kemaren tu123 ada kelinci di kebun\n",
      "After  : kemaren tu ada kelinci di kebun\n"
     ]
    }
   ],
   "source": [
    "sample_text = 'kemaren tu123 ada kelinci di kebun'\n",
    "\n",
    "print('Before :', sample_text)\n",
    "print('After  :', remove_non_alphabet(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove twitter & instagram formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_twitter_ig_formatting(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text)\n",
    "    text = re.sub(r'\\brt\\b', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before : @jonijon menurut saya hal tersebut masih kurang baik dilakukan sih kak\n",
      "After  :  menurut saya hal tersebut masih kurang baik dilakukan sih kak\n"
     ]
    }
   ],
   "source": [
    "sample_text = '@jonijon menurut saya hal tersebut masih kurang baik dilakukan sih kak'\n",
    "\n",
    "print('Before :', sample_text)\n",
    "print('After  :', remove_twitter_ig_formatting(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Repeating Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def remove_repeating_characters(text):\n",
    "    return ''.join(''.join(s)[:1] for _, s in itertools.groupby(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before : heyyyyyyyyyyyyyyyyyyyy kenapa tadi?\n",
      "After  : hey kenapa tadi?\n"
     ]
    }
   ],
   "source": [
    "sample_text = 'heyyyyyyyyyyyyyyyyyyyy kenapa tadi?'\n",
    "\n",
    "print('Before :', sample_text)\n",
    "print('After  :', remove_repeating_characters(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Preprocessing Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    transformed_text = text.lower()\n",
    "    transformed_text = remove_newline(text)\n",
    "    transformed_text = remove_url(transformed_text)\n",
    "    transformed_text = remove_twitter_ig_formatting(transformed_text)\n",
    "    transformed_text = remove_kaskus_formatting(transformed_text)\n",
    "    transformed_text = translate_emoticon(transformed_text)\n",
    "    transformed_text = transformed_text.lower()\n",
    "    transformed_text = tokenize_text(transformed_text)\n",
    "    transformed_text = transform_slang_words(transformed_text)\n",
    "    transformed_text = remove_repeating_characters(transformed_text)\n",
    "    transformed_text = transform_slang_words(transformed_text)\n",
    "    transformed_text = remove_non_alphabet(transformed_text)\n",
    "    transformed_text = remove_excessive_whitespace(transformed_text)\n",
    "    transformed_text = transformed_text.lower().strip()\n",
    "    return transformed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['processed_text'] = train_data['original_text'].apply(preprocess_text)\n",
    "test_data['processed_text'] = test_data['original_text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('{}/processed_train.csv'.format(PROCESSED_DATA_PATH), index=False)\n",
    "test_data.to_csv('{}/processed_test.csv'.format(PROCESSED_DATA_PATH), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
